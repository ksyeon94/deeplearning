{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   24.],\n",
       "       [   24.],\n",
       "       [   27.],\n",
       "       [   27.],\n",
       "       [   28.],\n",
       "       [   28.],\n",
       "       [   28.],\n",
       "       [   28.],\n",
       "       [   28.],\n",
       "       [   29.],\n",
       "       [   30.],\n",
       "       [   31.],\n",
       "       [   51.],\n",
       "       [  104.],\n",
       "       [  204.],\n",
       "       [  433.],\n",
       "       [  602.],\n",
       "       [  833.],\n",
       "       [  977.],\n",
       "       [ 1261.],\n",
       "       [ 1766.],\n",
       "       [ 2337.],\n",
       "       [ 3150.],\n",
       "       [ 3736.],\n",
       "       [ 4212.],\n",
       "       [ 4812.],\n",
       "       [ 5328.],\n",
       "       [ 5766.],\n",
       "       [ 6284.],\n",
       "       [ 6767.],\n",
       "       [ 7134.],\n",
       "       [ 7382.],\n",
       "       [ 7513.],\n",
       "       [ 7755.],\n",
       "       [ 7869.],\n",
       "       [ 7979.],\n",
       "       [ 8086.],\n",
       "       [ 8162.],\n",
       "       [ 8236.],\n",
       "       [ 8320.],\n",
       "       [ 8413.],\n",
       "       [ 8565.],\n",
       "       [ 8652.],\n",
       "       [ 8799.],\n",
       "       [ 8897.],\n",
       "       [ 8961.],\n",
       "       [ 9037.],\n",
       "       [ 9137.],\n",
       "       [ 9241.],\n",
       "       [ 9332.],\n",
       "       [ 9478.],\n",
       "       [ 9583.],\n",
       "       [ 9661.],\n",
       "       [ 9786.],\n",
       "       [ 9887.],\n",
       "       [ 9976.],\n",
       "       [10062.],\n",
       "       [10156.],\n",
       "       [10237.],\n",
       "       [10284.],\n",
       "       [10331.],\n",
       "       [10384.],\n",
       "       [10423.],\n",
       "       [10450.],\n",
       "       [10480.],\n",
       "       [10512.],\n",
       "       [10537.],\n",
       "       [10564.],\n",
       "       [10591.],\n",
       "       [10613.],\n",
       "       [10635.],\n",
       "       [10653.],\n",
       "       [10661.],\n",
       "       [10674.],\n",
       "       [10683.],\n",
       "       [10694.],\n",
       "       [10702.],\n",
       "       [10708.],\n",
       "       [10718.],\n",
       "       [10728.],\n",
       "       [10738.],\n",
       "       [10752.],\n",
       "       [10761.],\n",
       "       [10765.],\n",
       "       [10774.],\n",
       "       [10780.],\n",
       "       [10793.],\n",
       "       [10801.],\n",
       "       [10804.],\n",
       "       [10806.],\n",
       "       [10810.],\n",
       "       [10822.],\n",
       "       [10840.],\n",
       "       [10874.],\n",
       "       [10909.],\n",
       "       [10936.],\n",
       "       [10962.],\n",
       "       [10991.],\n",
       "       [11018.],\n",
       "       [11037.],\n",
       "       [11050.],\n",
       "       [11065.],\n",
       "       [11078.],\n",
       "       [11110.],\n",
       "       [11122.],\n",
       "       [11142.],\n",
       "       [11165.],\n",
       "       [11190.],\n",
       "       [11206.],\n",
       "       [11225.],\n",
       "       [11265.],\n",
       "       [11344.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = read_csv('/Users/gimseung-yeon/연습/deeplearning/corona_daily.csv', usecols=[3], engine='python', skipfooter=3)\n",
    "dataset=dataframe.values\n",
    "dataset=dataset.astype('float')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "dataset1=scaler.fit_transform(dataset) #정규화완료\n",
    "#훈련, 검증 셋으로 나누기\n",
    "train, test=train_test_split(dataset1, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 넣었을때 123->4, 234->5 예측하게 만들기 100 0~96, 99\n",
    "def 전처리(dataset, lookback):\n",
    "    입력값=[]\n",
    "    출력값=[]\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        입력값.append(dataset[i:(i+lookback),0])\n",
    "        출력값.append(dataset[i+lookback,0])\n",
    "    return np.array(입력값), np.array(출력값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 3) (86,) (20, 3) (20,)\n"
     ]
    }
   ],
   "source": [
    "# train, test dataset 만들기\n",
    "look_back=3\n",
    "x_train, y_train = 전처리(train,look_back)\n",
    "x_test, y_test = 전처리(test,look_back)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 1, 3) (20, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train= np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "X_test= np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 21        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25 (100.00 Byte)\n",
      "Trainable params: 25 (100.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(1, look_back)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 433us/step - loss: 0.4160\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 306us/step - loss: 0.0903\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 300us/step - loss: 0.0303\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 312us/step - loss: 0.0202\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 318us/step - loss: 0.0150\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 305us/step - loss: 0.0107\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 325us/step - loss: 0.0074\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 307us/step - loss: 0.0049\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 305us/step - loss: 0.0032\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 494us/step - loss: 0.0021\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 315us/step - loss: 0.0014\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 9.4603e-04\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 7.0641e-04\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 298us/step - loss: 5.7909e-04\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 5.6150e-04\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 318us/step - loss: 4.9888e-04\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 312us/step - loss: 4.6613e-04\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.7631e-04\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 4.5966e-04\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 295us/step - loss: 4.6339e-04\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 306us/step - loss: 4.5547e-04\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 301us/step - loss: 4.6821e-04\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.6176e-04\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.5360e-04\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.8029e-04\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 296us/step - loss: 4.7090e-04\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 297us/step - loss: 4.6246e-04\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 294us/step - loss: 4.5817e-04\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 4.6288e-04\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 297us/step - loss: 4.5449e-04\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 298us/step - loss: 4.4998e-04\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 355us/step - loss: 4.6621e-04\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 300us/step - loss: 4.5778e-04\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 297us/step - loss: 4.5707e-04\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 306us/step - loss: 4.4622e-04\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 4.4606e-04\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.6994e-04\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 300us/step - loss: 4.7223e-04\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 4.3789e-04\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 305us/step - loss: 4.4749e-04\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 4.6906e-04\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 326us/step - loss: 4.5025e-04\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 319us/step - loss: 4.6761e-04\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 298us/step - loss: 4.4759e-04\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.2894e-04\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 296us/step - loss: 4.3756e-04\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 295us/step - loss: 4.5573e-04\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 292us/step - loss: 4.3350e-04\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 289us/step - loss: 4.3798e-04\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 295us/step - loss: 4.5947e-04\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 4.3857e-04\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 296us/step - loss: 4.3466e-04\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 304us/step - loss: 4.2581e-04\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 4.0570e-04\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 551us/step - loss: 4.6345e-04\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 321us/step - loss: 4.1766e-04\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 313us/step - loss: 4.0076e-04\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 342us/step - loss: 4.3650e-04\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 4.0443e-04\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 311us/step - loss: 4.6347e-04\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 308us/step - loss: 4.2327e-04\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 3.8292e-04\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 304us/step - loss: 4.3346e-04\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 320us/step - loss: 3.7790e-04\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 307us/step - loss: 4.0461e-04\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 310us/step - loss: 4.0070e-04\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 300us/step - loss: 3.6106e-04\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 307us/step - loss: 3.8471e-04\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 308us/step - loss: 3.6290e-04\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 579us/step - loss: 3.6093e-04\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 306us/step - loss: 3.6636e-04\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 350us/step - loss: 3.7240e-04\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 327us/step - loss: 3.4930e-04\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 318us/step - loss: 3.4281e-04\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 332us/step - loss: 3.3129e-04\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 308us/step - loss: 3.5929e-04\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 300us/step - loss: 3.5979e-04\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 298us/step - loss: 3.4731e-04\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 300us/step - loss: 3.2422e-04\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 3.3393e-04\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 297us/step - loss: 3.3563e-04\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 308us/step - loss: 3.2295e-04\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 3.2843e-04\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 293us/step - loss: 2.9844e-04\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 306us/step - loss: 3.0192e-04\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 3.0039e-04\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 305us/step - loss: 2.9197e-04\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 3.0683e-04\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 295us/step - loss: 2.8490e-04\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 2.8976e-04\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 307us/step - loss: 2.9039e-04\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 2.9329e-04\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 320us/step - loss: 2.6760e-04\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 298us/step - loss: 2.5442e-04\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 303us/step - loss: 3.1327e-04\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 307us/step - loss: 2.9216e-04\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 298us/step - loss: 2.4806e-04\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 302us/step - loss: 2.9841e-04\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 304us/step - loss: 2.5359e-04\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 299us/step - loss: 2.4494e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15d40c4c0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=1, verbose=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 886us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "trainPredict=model.predict(X_train)\n",
    "testPredict=model.predict(X_test)\n",
    "#train과 test가 예측한 값과 실제값 확인하기\n",
    "trainPredict1=scaler.inverse_transform(trainPredict)\n",
    "Y_train = scaler.inverse_transform([y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.65017668e-04, 3.53356890e-04, 3.53356890e-04, 3.53356890e-04,\n",
       "        3.53356890e-04, 3.53356890e-04, 4.41696113e-04, 5.30035336e-04,\n",
       "        6.18374558e-04, 2.38515901e-03, 7.06713781e-03, 1.59010601e-02,\n",
       "        3.61307420e-02, 5.10600707e-02, 7.14664311e-02, 8.41872792e-02,\n",
       "        1.09275618e-01, 1.53886926e-01, 2.04328622e-01, 2.76148410e-01,\n",
       "        3.27915194e-01, 3.69964664e-01, 4.22968198e-01, 4.68551237e-01,\n",
       "        5.07243816e-01, 5.53003534e-01, 5.95671378e-01, 6.28091873e-01,\n",
       "        6.50000000e-01, 6.61572438e-01, 6.82950530e-01, 6.93021201e-01,\n",
       "        7.02738516e-01, 7.12190813e-01, 7.18904594e-01, 7.25441696e-01,\n",
       "        7.32862191e-01, 7.41077739e-01, 7.54505300e-01, 7.62190813e-01,\n",
       "        7.75176678e-01, 7.83833922e-01, 7.89487633e-01, 7.96201413e-01,\n",
       "        8.05035336e-01, 8.14222615e-01, 8.22261484e-01, 8.35159011e-01,\n",
       "        8.44434629e-01, 8.51325088e-01, 8.62367491e-01, 8.71289753e-01,\n",
       "        8.79151943e-01, 8.86749117e-01, 8.95053004e-01, 9.02208481e-01,\n",
       "        9.06360424e-01, 9.10512367e-01, 9.15194346e-01, 9.18639576e-01,\n",
       "        9.21024735e-01, 9.23674912e-01, 9.26501767e-01, 9.28710247e-01,\n",
       "        9.31095406e-01, 9.33480565e-01, 9.35424028e-01, 9.37367491e-01,\n",
       "        9.38957597e-01, 9.39664311e-01, 9.40812721e-01, 9.41607774e-01,\n",
       "        9.42579505e-01, 9.43286219e-01, 9.43816254e-01, 9.44699647e-01,\n",
       "        9.45583039e-01, 9.46466431e-01, 9.47703180e-01, 9.48498233e-01,\n",
       "        9.48851590e-01, 9.49646643e-01, 9.50176678e-01, 9.51325088e-01,\n",
       "        9.52031802e-01, 9.52296820e-01])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   27,    28,    28,    28,    28,    28,    29,    30,    31,\n",
       "           51,   104,   204,   433,   602,   832,   977,  1260,  1766,\n",
       "         2337,  3150,  3736,  4212,  4812,  5328,  5766,  6283,  6767,\n",
       "         7134,  7382,  7513,  7755,  7868,  7979,  8086,  8162,  8236,\n",
       "         8320,  8413,  8565,  8652,  8799,  8897,  8961,  9037,  9137,\n",
       "         9241,  9332,  9478,  9583,  9661,  9786,  9887,  9976, 10062,\n",
       "        10156, 10237, 10284, 10331, 10384, 10423, 10450, 10480, 10512,\n",
       "        10537, 10564, 10591, 10613, 10635, 10653, 10661, 10674, 10683,\n",
       "        10694, 10702, 10708, 10718, 10728, 10738, 10752, 10761, 10765,\n",
       "        10774, 10780, 10793, 10801, 10804]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  144],\n",
       "       [  144],\n",
       "       [  146],\n",
       "       [  146],\n",
       "       [  146],\n",
       "       [  146],\n",
       "       [  146],\n",
       "       [  147],\n",
       "       [  148],\n",
       "       [  149],\n",
       "       [  168],\n",
       "       [  218],\n",
       "       [  316],\n",
       "       [  542],\n",
       "       [  714],\n",
       "       [  970],\n",
       "       [ 1130],\n",
       "       [ 1439],\n",
       "       [ 1936],\n",
       "       [ 2508],\n",
       "       [ 3335],\n",
       "       [ 3939],\n",
       "       [ 4498],\n",
       "       [ 5119],\n",
       "       [ 5623],\n",
       "       [ 6086],\n",
       "       [ 6594],\n",
       "       [ 7037],\n",
       "       [ 7404],\n",
       "       [ 7678],\n",
       "       [ 7844],\n",
       "       [ 8076],\n",
       "       [ 8168],\n",
       "       [ 8299],\n",
       "       [ 8395],\n",
       "       [ 8467],\n",
       "       [ 8540],\n",
       "       [ 8612],\n",
       "       [ 8688],\n",
       "       [ 8808],\n",
       "       [ 8875],\n",
       "       [ 9009],\n",
       "       [ 9082],\n",
       "       [ 9154],\n",
       "       [ 9225],\n",
       "       [ 9301],\n",
       "       [ 9379],\n",
       "       [ 9454],\n",
       "       [ 9568],\n",
       "       [ 9642],\n",
       "       [ 9720],\n",
       "       [ 9820],\n",
       "       [ 9890],\n",
       "       [ 9968],\n",
       "       [10039],\n",
       "       [10111],\n",
       "       [10173],\n",
       "       [10218],\n",
       "       [10264],\n",
       "       [10304],\n",
       "       [10333],\n",
       "       [10360],\n",
       "       [10386],\n",
       "       [10409],\n",
       "       [10428],\n",
       "       [10450],\n",
       "       [10469],\n",
       "       [10487],\n",
       "       [10505],\n",
       "       [10519],\n",
       "       [10527],\n",
       "       [10539],\n",
       "       [10545],\n",
       "       [10554],\n",
       "       [10560],\n",
       "       [10566],\n",
       "       [10573],\n",
       "       [10579],\n",
       "       [10587],\n",
       "       [10596],\n",
       "       [10602],\n",
       "       [10608],\n",
       "       [10615],\n",
       "       [10618],\n",
       "       [10628],\n",
       "       [10632]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPredict1.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
